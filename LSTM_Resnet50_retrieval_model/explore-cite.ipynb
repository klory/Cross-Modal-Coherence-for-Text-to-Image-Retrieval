{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pretty printing has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pprint on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import utils\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "import json\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from types import SimpleNamespace\n",
    "from networks import TextEncoder, ImageEncoder, DiscourseClassifier\n",
    "from datasets import CoherenceDataset, val_transform\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "args = {\n",
    "    'data_source': 'recipe',\n",
    "    'img_dir': '../data/RecipeQA/images-qa/train/images-qa',\n",
    "    'word2vec_dim': 300,\n",
    "    'rnn_hid_dim': 300,\n",
    "    'feature_dim': 1024,\n",
    "    'max_len': 200,\n",
    "    'dataset_q': 0,\n",
    "    'with_attention': 2,\n",
    "    'batch_size': 64,\n",
    "    'workers': 4\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "relations = ['q2_resp', 'q3_resp', 'q4_resp', 'q5_resp', 'q6_resp', 'q7_resp', 'q8_resp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size = 8918\n",
      "train data: 3439 54\n"
     ]
    }
   ],
   "source": [
    "train_set = CoherenceDataset(\n",
    "            part='train',\n",
    "            datasource=args.data_source,\n",
    "            word2vec_file=f'models/word2vec_{args.data_source}.bin',\n",
    "            max_len=args.max_len,\n",
    "            dataset_q=args.dataset_q,  # experimental things, ignore it for now\n",
    "            transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "            train_set, batch_size=args.batch_size, shuffle=False,\n",
    "            num_workers=args.workers, pin_memory=True,\n",
    "            drop_last=False)\n",
    "\n",
    "print('train data:', len(train_set), len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size = 8918\n",
      "test data: 860 14\n"
     ]
    }
   ],
   "source": [
    "test_set = CoherenceDataset(\n",
    "            part='test',\n",
    "            datasource=args.data_source,\n",
    "            word2vec_file=f'models/word2vec_{args.data_source}.bin',\n",
    "            max_len=args.max_len,\n",
    "            dataset_q=args.dataset_q,  # experimental things, ignore it for now\n",
    "            transform=val_transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "            test_set, batch_size=args.batch_size, shuffle=False,\n",
    "            num_workers=args.workers, pin_memory=True,\n",
    "            drop_last=False)\n",
    "\n",
    "print('test data:', len(test_set), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21777620396600567, 7.684343434343434, 2.043362831858407, 4.171428571428572, 5.333333333333333, 0.7016328550222662, 2.196096654275093]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.n2p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    text_encoder = TextEncoder(\n",
    "        emb_dim=args.word2vec_dim,\n",
    "        hid_dim=args.rnn_hid_dim,\n",
    "        z_dim=args.feature_dim,\n",
    "        max_len = args.max_len,\n",
    "        word2vec_file=f'models/word2vec_{args.data_source}.bin',\n",
    "        with_attention=args.with_attention).to(device)\n",
    "    image_encoder = ImageEncoder(\n",
    "        z_dim=args.feature_dim).to(device)\n",
    "    discourse_class = DiscourseClassifier(\n",
    "        len(relations), args.feature_dim).to(device)\n",
    "\n",
    "    ckpt = torch.load(path)\n",
    "    text_encoder.load_state_dict(ckpt['text_encoder'])\n",
    "    image_encoder.load_state_dict(ckpt['image_encoder'])\n",
    "    discourse_class.load_state_dict(ckpt['discourse_class'])\n",
    "    return text_encoder, image_encoder, discourse_class\n",
    "\n",
    "path_base = 'runs/samples3439_retrieval=1.00_classification=0.00_reweight=1000.00_weightDecay=0.0_withAttention=2_question=2,3,4,5,6,7,8_maxLen=200/e19.ckpt'\n",
    "path_all = 'runs/samples3439_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=2,3,4,5,6,7,8_maxLen=200/e19.ckpt'\n",
    "\n",
    "t_base, i_base, d_base = load_model(path_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:09<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_output(test_loader, text_encoder, image_encoder, discourse_class, valid_questions):\n",
    "    txt_feats = []\n",
    "    img_feats = []\n",
    "    probs = []\n",
    "    labels = []\n",
    "    attns = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        for i in range(len(batch)):\n",
    "            batch[i] = batch[i].to(device)\n",
    "        txt, txt_len, img, target = batch\n",
    "        with torch.no_grad():\n",
    "            txt_feat, attn = text_encoder(txt.long(), txt_len)\n",
    "            img_feat = image_encoder(img)\n",
    "            prob = torch.sigmoid(discourse_class(txt_feat, img_feat))[:,valid_questions]\n",
    "            txt_feats.append(txt_feat.detach().cpu())\n",
    "            img_feats.append(img_feat.detach().cpu())\n",
    "            probs.append(prob.detach().cpu())\n",
    "            attns.append(attn.detach().cpu())\n",
    "            labels.append(target[:,valid_questions].detach().cpu())\n",
    "\n",
    "    txt_feats = torch.cat(txt_feats, dim=0).numpy()\n",
    "    img_feats = torch.cat(img_feats, dim=0).numpy()\n",
    "    probs = torch.cat(probs, dim=0).numpy()\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    attns = torch.cat(attns, dim=0).numpy()\n",
    "    return probs, labels, attns, txt_feats, img_feats\n",
    "\n",
    "valid_questions_base = torch.tensor([0,1,2,3,4,5,6], dtype=torch.long) # base\n",
    "probs_base, labels_base, attns_base, txt_base, img_base = generate_output(test_loader, t_base, i_base, d_base, valid_questions_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which single-discourse model to load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_id = 2\n",
    "path = f'runs/samples3439_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question={q_id}_maxLen=200/e19.ckpt'\n",
    "relation = f'q{q_id}'\n",
    "relation_idx = q_id-2\n",
    "valid_questions_q = torch.tensor([relation_idx], dtype=torch.long) # Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "t_q, i_q, d_q = load_model(path)\n",
    "probs_q, labels_q, attns_q, txt_q, img_q = generate_output(test_loader, t_q, i_q, d_q, valid_questions_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare baseline with single-discourse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(860, 860)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks_base = utils.compute_ranks(txt_base, img_base)\n",
    "ranks_q = utils.compute_ranks(txt_q, img_q)\n",
    "ranks_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = probs_q\n",
    "labels = labels_q\n",
    "attns = attns_q\n",
    "ranks = ranks_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(860, 1) (860, 1) (860, 200) (860, 860) (860,)\n",
      "(860, 7) (860, 7) (860, 200) (860, 860) (860,)\n"
     ]
    }
   ],
   "source": [
    "def get_pos(ranks):\n",
    "    out = []\n",
    "    for ii, rank in enumerate(ranks):\n",
    "        pos = rank.tolist().index(ii)\n",
    "        out.append(pos)\n",
    "    return np.asarray(out)\n",
    "\n",
    "positions = get_pos(ranks)\n",
    "positions_base = get_pos(ranks_base)\n",
    "\n",
    "print(probs.shape, labels.shape, attns.shape, ranks.shape, positions.shape)\n",
    "print(probs_base.shape, labels_base.shape, attns_base.shape, ranks_base.shape, positions_base.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discourse is True or False?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(687,)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 1\n",
    "t_indices = np.where(labels==T)[0]\n",
    "t_positions = positions[t_indices]\n",
    "tmp = np.argsort(t_positions, 0).squeeze()\n",
    "t_indices = t_indices[tmp]\n",
    "t_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which sample to show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgheader = 'qimg'\n",
    "capheader = 'stim_txt'\n",
    "img_dir = '../data/RecipeQA/images-qa/train/images-qa'\n",
    "save_dir = f'outputs/cite/{relation}={T}/coherence_win'\n",
    "save_dir = f'outputs/cite/{relation}={T}/coherence_lose'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_indices = []\n",
    "for a,b,idx in zip(positions[t_indices], positions_base[t_indices], t_indices):\n",
    "#     if a<5 and b-a >= 5:\n",
    "    if b<5 and b-a >= 3:\n",
    "#         print(a,b, idx)\n",
    "        good_indices.append(idx)\n",
    "len(good_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(save_dir, 'captions.txt'), 'w') as f:\n",
    "    for idx in tqdm(good_indices[:20]):\n",
    "        rcp = test_set.recipes.iloc[idx]\n",
    "        cap = rcp[capheader]\n",
    "        f.write(f'{idx:>5d} ' + cap + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "my_trans = transforms.Compose([\n",
    "    transforms.Resize(512),\n",
    "    transforms.CenterCrop(512)\n",
    "])\n",
    "\n",
    "def show_img(rcp, title=None):\n",
    "    img_path = os.path.join(img_dir, rcp[imgheader])\n",
    "    img = my_trans(Image.open(img_path))\n",
    "#     _ = plt.imshow(np.asarray(img))\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        img.save(title, 'JPEG')\n",
    "        \n",
    "for idx in tqdm(good_indices[:20]):\n",
    "    # idx = 489\n",
    "\n",
    "    sub_dir = os.path.join(save_dir, str(idx))\n",
    "    os.makedirs(sub_dir, exist_ok=True)\n",
    "    f = open(os.path.join(sub_dir, 'attentions.txt'), 'w')\n",
    "\n",
    "    rcp = test_set.recipes.iloc[idx]\n",
    "    cap = rcp[capheader]\n",
    "    cap = utils.clean_caption(cap)\n",
    "    words = re.split(r'\\\\n| ', cap)[:args.max_len]\n",
    "    # print(cap)\n",
    "    # print(words)\n",
    "\n",
    "#     print(f'\\n==> {relation} == {T}')\n",
    "#     print(probs[idx])\n",
    "    f.write(f'Model: {relation} is used\\n')\n",
    "    f.write(f'prob = {probs[idx][0]:.4f}\\n')\n",
    "    for w, attn in zip(words, attns[idx][:len(words)]):\n",
    "        line = f'{w:>20s} = {attn:<.4f}'\n",
    "#         print(line)\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "#     print('\\n==> no relation')\n",
    "#     print(probs_all[idx][relation_idx])\n",
    "    f.write(f'\\nModel: no relation is used\\n')\n",
    "    f.write(f'prob = {probs_base[idx][relation_idx]:.4f}\\n')\n",
    "    for w, attn in zip(words, attns_base[idx][:len(words)]):\n",
    "        line = f'{w:>20s} = {attn:<.4f}'\n",
    "#         print(line)\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "\n",
    "#     show_img(rcp, title=os.path.join(sub_dir, f'real.jpg'))\n",
    "\n",
    "    # ranks\n",
    "    rank = ranks[idx]\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    pos = rank.tolist().index(idx)\n",
    "    line = f'{relation} == {T}. Top 5 retrieved images, while true image is at pos={pos}'\n",
    "    fig.suptitle(line, y=0.7)\n",
    "    f.write(line+'\\n')\n",
    "    i = 0\n",
    "    for idx_ in rank[:5]:\n",
    "        plt.subplot(151+i)\n",
    "        show_img(test_set.recipes.iloc[idx_], title=os.path.join(sub_dir, f'{relation}_top{i}.jpg'))\n",
    "        i+=1\n",
    "\n",
    "    rank_base = ranks_base[idx]\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    pos = rank_base.tolist().index(idx)\n",
    "    line = f'No Relation. Top 5 retrieved images, while true image is at pos={pos}'\n",
    "    fig.suptitle(line, y=0.7)\n",
    "    f.write(line+'\\n')\n",
    "    i = 0\n",
    "    for idx_ in rank_base[:5]:\n",
    "        plt.subplot(151+i)\n",
    "        show_img(test_set.recipes.iloc[idx_], title=os.path.join(sub_dir, f'no_relation_top{i}.jpg'))\n",
    "        i+=1\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Human Rate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(687,)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_indices = np.where(labels==T)[0]\n",
    "t_positions = positions[t_indices]\n",
    "tmp = np.argsort(t_positions, 0).squeeze()\n",
    "t_indices = t_indices[tmp]\n",
    "t_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/cite/q2=1/human_evaluation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = f'outputs/cite/{relation}={T}/human_evaluation'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "good_indices = []\n",
    "for a,b,idx in zip(positions[t_indices], positions_base[t_indices], t_indices):\n",
    "    if b-a >0:\n",
    "        good_indices.append(idx)\n",
    "\n",
    "print(save_dir)\n",
    "len(good_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 3723.90it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(save_dir, 'captions.txt'), 'w') as f:\n",
    "    for idx in tqdm(good_indices[:100]):\n",
    "        rcp = test_set.recipes.iloc[idx]\n",
    "        cap = rcp[capheader]\n",
    "        f.write(f'{idx:>5d} ' + cap + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 24.68it/s]\n"
     ]
    }
   ],
   "source": [
    "my_trans = transforms.Compose([\n",
    "    transforms.Resize(512),\n",
    "    transforms.CenterCrop(512)\n",
    "])\n",
    "\n",
    "\n",
    "def show_img(rcp, title=None):\n",
    "    img_path = os.path.join(img_dir, rcp[imgheader])\n",
    "    img = my_trans(Image.open(img_path))\n",
    "    if title:\n",
    "        img.save(title, 'JPEG')\n",
    "\n",
    "\n",
    "for idx in tqdm(good_indices[:100]):\n",
    "    rcp = test_set.recipes.iloc[idx]\n",
    "\n",
    "    rank = ranks[idx]\n",
    "    for idx_ in rank[:1]:\n",
    "        show_img(test_set.recipes.iloc[idx_], title=os.path.join(save_dir, f'{idx}_cohaware.jpg'))\n",
    "\n",
    "\n",
    "    rank_base = ranks_base[idx]\n",
    "    for idx_ in rank_base[:1]:\n",
    "        show_img(test_set.recipes.iloc[idx_], title=os.path.join(save_dir, f'{idx}_cohagnostic.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd08d113a18ebd6a42e356910a7d46060f616cc21ad88414157dc69df48308f9ac5",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}