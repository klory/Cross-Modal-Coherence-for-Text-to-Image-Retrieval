{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pprint on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import utils\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "import json\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from types import SimpleNamespace\n",
    "from networks import TextEncoder, ImageEncoder, DiscourseClassifier\n",
    "from datasets import CoherenceDataset, val_transform\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import transforms\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, with_attention=2):\n",
    "    text_encoder = TextEncoder(\n",
    "        emb_dim=args.word2vec_dim,\n",
    "        hid_dim=args.rnn_hid_dim,\n",
    "        z_dim=args.feature_dim,\n",
    "        max_len = args.max_len,\n",
    "        word2vec_file=f'models/word2vec_{args.data_source}.bin',\n",
    "        with_attention=with_attention).to(device)\n",
    "    image_encoder = ImageEncoder(\n",
    "        z_dim=args.feature_dim).to(device)\n",
    "    discourse_class = DiscourseClassifier(\n",
    "        len(relations), args.feature_dim).to(device)\n",
    "\n",
    "    ckpt = torch.load(path)\n",
    "    text_encoder.load_state_dict(ckpt['text_encoder'])\n",
    "    image_encoder.load_state_dict(ckpt['image_encoder'])\n",
    "    discourse_class.load_state_dict(ckpt['discourse_class'])\n",
    "    return text_encoder, image_encoder, discourse_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(test_loader, text_encoder, image_encoder, discourse_class, valid_questions):\n",
    "    txt_feats = []\n",
    "    img_feats = []\n",
    "    probs = []\n",
    "    labels = []\n",
    "    attns = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        for i in range(len(batch)):\n",
    "            batch[i] = batch[i].to(device)\n",
    "        txt, txt_len, img, target = batch\n",
    "        with torch.no_grad():\n",
    "            txt_feat, attn = text_encoder(txt.long(), txt_len)\n",
    "            img_feat = image_encoder(img)\n",
    "            prob = torch.sigmoid(discourse_class(txt_feat, img_feat))[:,valid_questions]\n",
    "            txt_feats.append(txt_feat.detach().cpu())\n",
    "            img_feats.append(img_feat.detach().cpu())\n",
    "            probs.append(prob.detach().cpu())\n",
    "            if text_encoder.with_attention:\n",
    "                attns.append(attn.detach().cpu())\n",
    "            labels.append(target[:,valid_questions].detach().cpu())\n",
    "\n",
    "    txt_feats = torch.cat(txt_feats, dim=0).numpy()\n",
    "    img_feats = torch.cat(img_feats, dim=0).numpy()\n",
    "    probs = torch.cat(probs, dim=0).numpy()\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    if text_encoder.with_attention:\n",
    "        attns = torch.cat(attns, dim=0).numpy()\n",
    "    return probs, labels, attns, txt_feats, img_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(rcps, imgs, retrieved_type='image', retrieved_range=1000, n_repeat=30):\n",
    "    N = retrieved_range\n",
    "    data_size = imgs.shape[0]\n",
    "    idxs = range(N)\n",
    "    glob_rank = []\n",
    "    glob_recall = {1: 0.0, 5: 0.0, 10: 0.0}\n",
    "    # average over 20 sets\n",
    "    for i in range(n_repeat):\n",
    "        ids_sub = np.random.choice(data_size, N, replace=False)\n",
    "        imgs_sub = imgs[ids_sub, :]\n",
    "        rcps_sub = rcps[ids_sub, :]\n",
    "        imgs_sub = imgs_sub / np.linalg.norm(imgs_sub, axis=1)[:, None]\n",
    "        rcps_sub = rcps_sub / np.linalg.norm(rcps_sub, axis=1)[:, None]\n",
    "        if retrieved_type == 'recipe':\n",
    "            sims = np.dot(imgs_sub, rcps_sub.T)  # [N, N]\n",
    "        else:\n",
    "            sims = np.dot(rcps_sub, imgs_sub.T)\n",
    "        med_rank = []\n",
    "        recall = {1: 0.0, 5: 0.0, 10: 0.0}\n",
    "        # loop through the N similarities for images\n",
    "        for ii in idxs:\n",
    "            # get a column of similarities for image ii\n",
    "            sim = sims[ii, :]\n",
    "            # sort indices in descending order\n",
    "            sorting = np.argsort(sim)[::-1].tolist()\n",
    "            # find where the index of the pair sample ended up in the sorting\n",
    "            pos = sorting.index(ii)\n",
    "            if (pos + 1) == 1:\n",
    "                recall[1] += 1\n",
    "            if (pos + 1) <= 5:\n",
    "                recall[5] += 1\n",
    "            if (pos + 1) <= 10:\n",
    "                recall[10] += 1\n",
    "            # store the position\n",
    "            med_rank.append(pos+1)\n",
    "\n",
    "        for i in recall.keys():\n",
    "            recall[i] = recall[i]/N\n",
    "        med = np.median(med_rank)\n",
    "        for i in recall.keys():\n",
    "            glob_recall[i] += recall[i]\n",
    "        glob_rank.append(med)\n",
    "\n",
    "    for i in glob_recall.keys():\n",
    "        glob_recall[i] = glob_recall[i]/10\n",
    "    return np.asarray(glob_rank), glob_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "args = {\n",
    "    'data_source': 'cite',\n",
    "    'img_dir': '../data/RecipeQA/images-qa/train/images-qa',\n",
    "    'word2vec_dim': 300,\n",
    "    'rnn_hid_dim': 300,\n",
    "    'feature_dim': 1024,\n",
    "    'max_len': 200,\n",
    "    'dataset_q': 0,\n",
    "    'with_attention': 2,\n",
    "    'batch_size': 64,\n",
    "    'workers': 4\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "relations = ['q2_resp', 'q3_resp', 'q4_resp', 'q5_resp', 'q6_resp', 'q7_resp', 'q8_resp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size = 8918\n",
      "test data: 860 14\n"
     ]
    }
   ],
   "source": [
    "test_set = CoherenceDataset(\n",
    "            part='test',\n",
    "            datasource=args.data_source,\n",
    "            word2vec_file=f'models/word2vec_{args.data_source}.bin',\n",
    "            max_len=args.max_len,\n",
    "            dataset_q=args.dataset_q,  # experimental things, ignore it for now\n",
    "            transform=val_transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "            test_set, batch_size=args.batch_size, shuffle=False,\n",
    "            num_workers=args.workers, pin_memory=True,\n",
    "            drop_last=False)\n",
    "\n",
    "print('test data:', len(test_set), len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxplot the performances of different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, name, path, questions, with_attention):\n",
    "        self.name = name\n",
    "        self.path = path\n",
    "        self.questions = questions\n",
    "        self.with_attention = with_attention\n",
    "        self.medRs = None\n",
    "        self.recalls = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CITE++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = []\n",
    "# Base\n",
    "path = 'runs/samples3439_retrieval=1.00_classification=0.00_reweight=1000.00_weightDecay=0.0_withAttention=0_question=2,3,4,5,6,7,8_maxLen=200/e10.ckpt'\n",
    "valid_questions = torch.tensor([0,1,2,3,4,5,6], dtype=torch.long)\n",
    "with_attention = 0\n",
    "configs.append(Config('Base', path, valid_questions, with_attention))\n",
    "\n",
    "# CMCA\n",
    "path = 'runs/samples3439_retrieval=1.00_classification=0.00_reweight=1000.00_weightDecay=0.0_withAttention=2_question=2,3,4,5,6,7,8_maxLen=200/e10.ckpt'\n",
    "valid_questions = torch.tensor([0,1,2,3,4,5,6], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config('CMCA', path, valid_questions, with_attention))\n",
    "\n",
    "# CMCM-NoAttn\n",
    "path = 'runs/samples3439_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=0_question=2,3,4,5,6,7,8_maxLen=200/e10.ckpt'\n",
    "valid_questions = torch.tensor([0,1,2,3,4,5,6], dtype=torch.long)\n",
    "with_attention = 0\n",
    "configs.append(Config('CMCM-NoAttn', path, valid_questions, with_attention))\n",
    "\n",
    "# CMCM\n",
    "path = 'runs/samples3439_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=2,3,4,5,6,7,8_maxLen=200/e10.ckpt'\n",
    "valid_questions = torch.tensor([0,1,2,3,4,5,6], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config('CMCM', path, valid_questions, with_attention))\n",
    "\n",
    "# Q2\n",
    "path = 'runs/samples3439_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=2_maxLen=200/e10.ckpt'\n",
    "valid_questions = torch.tensor([0], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config(r'CMCM_{Q2}', path, valid_questions, with_attention))\n",
    "\n",
    "# Q3\n",
    "path = 'runs/samples3439_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=3_maxLen=200/e10.ckpt'\n",
    "valid_questions = torch.tensor([1], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config(r'CMCM_{Q3}', path, valid_questions, with_attention))\n",
    "\n",
    "# Q4\n",
    "path = 'runs/samples3439_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=4_maxLen=200/e10.ckpt'\n",
    "valid_questions = torch.tensor([2], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config(r'CMCM_{Q4}', path, valid_questions, with_attention))\n",
    "\n",
    "# Q5\n",
    "path = 'runs/samples3439_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=5_maxLen=200/e10.ckpt'\n",
    "valid_questions = torch.tensor([3], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config(r'CMCM_{Q5}', path, valid_questions, with_attention))\n",
    "\n",
    "# Q6\n",
    "path = 'runs/samples3439_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=6_maxLen=200/e10.ckpt'\n",
    "valid_questions = torch.tensor([4], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config(r'CMCM_{Q6}', path, valid_questions, with_attention))\n",
    "\n",
    "# Q7\n",
    "path = 'runs/samples3439_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=7_maxLen=200/e10.ckpt'\n",
    "valid_questions = torch.tensor([5], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config(r'CMCM_{Q7}', path, valid_questions, with_attention))\n",
    "\n",
    "# Q8\n",
    "path = 'runs/samples3439_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=8_maxLen=200/e10.ckpt'\n",
    "valid_questions = torch.tensor([6], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config(r'CMCM_{Q8}', path, valid_questions, with_attention))\n",
    "\n",
    "len(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n"
     ]
    }
   ],
   "source": [
    "for config in configs:\n",
    "    txt_encoder, img_encoder, classifier = load_model(config.path, with_attention=config.with_attention)\n",
    "    probs_test, labels_test, attns_test, txt_test, img_test = generate_output(test_loader, txt_encoder, img_encoder, classifier, config.questions)\n",
    "    probs_test.shape\n",
    "    retrieved_range = min(txt_test.shape[0], 500)\n",
    "    print('retrieved_range =', retrieved_range)\n",
    "    medRs, recalls = rank(\n",
    "        txt_test, \n",
    "        img_test, \n",
    "        retrieved_type='image', \n",
    "        retrieved_range=retrieved_range,\n",
    "        n_repeat=50)\n",
    "    config.medRs = medRs\n",
    "    config.recalls = recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1152 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for config in configs:\n",
    "    df[config.name] = config.medRs\n",
    "#     print(f'\\n {config.name}')\n",
    "#     print(config.medRs.mean(), config.medRs.std())\n",
    "#     for r,v in config.recalls.items():\n",
    "#         print(r,v)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "df.plot.box(subplots=False, figsize=(12,6))\n",
    "ticks = range(1, len(configs)+1)\n",
    "labels = [r'${}$'.format(x.name) for x in configs]\n",
    "plt.xticks(ticks, labels)\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = \"10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
