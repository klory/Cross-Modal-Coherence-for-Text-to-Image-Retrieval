{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pprint on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import utils\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "import json\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from types import SimpleNamespace\n",
    "from networks import TextEncoder, ImageEncoder, DiscourseClassifier\n",
    "from datasets import CoherenceDataset, val_transform\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "args = {\n",
    "    'data_source': 'conceptual',\n",
    "    'img_dir': '../data/conceptual/images/',\n",
    "    'word2vec_dim': 300,\n",
    "    'rnn_hid_dim': 300,\n",
    "    'feature_dim': 1024,\n",
    "    'max_len': 40,\n",
    "    'dataset_q': 0,\n",
    "    'with_attention': 2,\n",
    "    'batch_size': 64,\n",
    "    'workers': 4\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "relations = relations = ['Visible', 'Subjective', 'Action', 'Story', 'Meta', 'Irrelevant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size = 5612\n",
      "test data: 1512 24\n"
     ]
    }
   ],
   "source": [
    "test_set = CoherenceDataset(\n",
    "            part='test',\n",
    "            datasource=args.data_source,\n",
    "            img_dir=args.img_dir,\n",
    "            word2vec_file=f'models/word2vec_{args.data_source}.bin',\n",
    "            max_len=args.max_len,\n",
    "            dataset_q=args.dataset_q,  # experimental things, ignore it for now\n",
    "            transform=val_transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "            test_set, batch_size=args.batch_size, shuffle=False,\n",
    "            num_workers=args.workers, pin_memory=True,\n",
    "            drop_last=False)\n",
    "\n",
    "print('test data:', len(test_set), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5074775672981057, 15.085106382978724, 5.406779661016949, 3.0427807486631018, 1.597938144329897, 9.5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.n2p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    text_encoder = TextEncoder(\n",
    "        emb_dim=args.word2vec_dim,\n",
    "        hid_dim=args.rnn_hid_dim,\n",
    "        z_dim=args.feature_dim,\n",
    "        max_len = args.max_len,\n",
    "        word2vec_file=f'models/word2vec_{args.data_source}.bin',\n",
    "        with_attention=args.with_attention).to(device)\n",
    "    image_encoder = ImageEncoder(\n",
    "        z_dim=args.feature_dim).to(device)\n",
    "    discourse_class = DiscourseClassifier(\n",
    "        len(relations), args.feature_dim).to(device)\n",
    "\n",
    "    ckpt = torch.load(path)\n",
    "    text_encoder.load_state_dict(ckpt['text_encoder'])\n",
    "    image_encoder.load_state_dict(ckpt['image_encoder'])\n",
    "    discourse_class.load_state_dict(ckpt['discourse_class'])\n",
    "    return text_encoder, image_encoder, discourse_class\n",
    "\n",
    "# path_vis = 'runs/samples6047_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=0_maxLen=40/e14.ckpt'\n",
    "\n",
    "# path_sub = 'runs/samples6047_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=1_maxLen=40/e14.ckpt'\n",
    "\n",
    "# path_act = 'runs/samples6047_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=2_maxLen=40/e14.ckpt'\n",
    "\n",
    "# path_sto = 'runs/samples6047_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=3_maxLen=40/e14.ckpt'\n",
    "\n",
    "path_met = 'runs/samples6047_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=4_maxLen=40/e14.ckpt'\n",
    "path_irr = 'runs/samples6047_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=5_maxLen=40/e14.ckpt'\n",
    "\n",
    "path_all = 'runs/samples6047_retrieval=1.00_classification=0.00_reweight=1000.00_weightDecay=0.0_withAttention=2_question=0,1,2,3,4,5_maxLen=40/e14.ckpt'\n",
    "\n",
    "# t_vis, i_vis, d_vis = load_model(path_vis)\n",
    "# t_sub, i_sub, d_sub = load_model(path_sub)\n",
    "# t_act, i_act, d_act = load_model(path_act)\n",
    "# t_sto, i_sto, d_sto = load_model(path_sto)\n",
    "\n",
    "t_met, i_met, d_met = load_model(path_met)\n",
    "t_irr, i_irr, d_irr = load_model(path_irr)\n",
    "\n",
    "t_all, i_all, d_all = load_model(path_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(test_loader, text_encoder, image_encoder, discourse_class, valid_questions):\n",
    "    txt_feats = []\n",
    "    img_feats = []\n",
    "    probs = []\n",
    "    labels = []\n",
    "    attns = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        for i in range(len(batch)):\n",
    "            batch[i] = batch[i].to(device)\n",
    "        txt, txt_len, img, target = batch\n",
    "        with torch.no_grad():\n",
    "            txt_feat, attn = text_encoder(txt.long(), txt_len)\n",
    "            img_feat = image_encoder(img)\n",
    "            prob = torch.sigmoid(discourse_class(txt_feat, img_feat))[:,valid_questions]\n",
    "            txt_feats.append(txt_feat.detach().cpu())\n",
    "            img_feats.append(img_feat.detach().cpu())\n",
    "            probs.append(prob.detach().cpu())\n",
    "            attns.append(attn.detach().cpu())\n",
    "            labels.append(target[:,valid_questions].detach().cpu())\n",
    "\n",
    "    txt_feats = torch.cat(txt_feats, dim=0).numpy()\n",
    "    img_feats = torch.cat(img_feats, dim=0).numpy()\n",
    "    probs = torch.cat(probs, dim=0).numpy()\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    attns = torch.cat(attns, dim=0).numpy()\n",
    "    return probs, labels, attns, txt_feats, img_feats\n",
    "\n",
    "# valid_questions_vis = torch.tensor([0], dtype=torch.long) # Visible\n",
    "# valid_questions_sub = torch.tensor([1], dtype=torch.long) # Subjective\n",
    "# valid_questions_act = torch.tensor([2], dtype=torch.long) # Action\n",
    "# valid_questions_sto = torch.tensor([3], dtype=torch.long) # Story\n",
    "\n",
    "valid_questions_met = torch.tensor([4], dtype=torch.long) # Meta\n",
    "valid_questions_irr = torch.tensor([5], dtype=torch.long) # Irrelavent\n",
    "\n",
    "valid_questions_all = torch.tensor([0,1,2,3,4,5], dtype=torch.long) # all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:50<00:00,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "probs_vis, labels_vis, attns_vis, txt_vis, img_vis = generate_output(test_loader, t_vis, i_vis, d_vis, valid_questions_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:47<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "probs_sub, labels_sub, attns_sub, txt_sub, img_sub = generate_output(test_loader, t_sub, i_sub, d_sub, valid_questions_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:47<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "probs_act, labels_act, attns_act, txt_act, img_act = generate_output(test_loader, t_act, i_act, d_act, valid_questions_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:46<00:00,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "probs_sto, labels_sto, attns_sto, txt_sto, img_sto = generate_output(test_loader, t_sto, i_sto, d_sto, valid_questions_sto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [01:49<00:00,  4.56s/it]\n"
     ]
    }
   ],
   "source": [
    "probs_met, labels_met, attns_met, txt_met, img_met = generate_output(test_loader, t_met, i_met, d_met, valid_questions_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:50<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "probs_irr, labels_irr, attns_irr, txt_irr, img_irr = generate_output(test_loader, t_irr, i_irr, d_irr, valid_questions_irr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:57<00:00,  2.41s/it]\n"
     ]
    }
   ],
   "source": [
    "probs_all, labels_all, attns_all, txt_all, img_all = generate_output(test_loader, t_all, i_all, d_all, valid_questions_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1512, 1512)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ranks_vis = utils.compute_ranks(txt_vis, img_vis)\n",
    "# ranks_sub = utils.compute_ranks(txt_sub, img_sub)\n",
    "# ranks_act = utils.compute_ranks(txt_act, img_act)\n",
    "# ranks_sto = utils.compute_ranks(txt_sto, img_sto)\n",
    "ranks_met = utils.compute_ranks(txt_met, img_met)\n",
    "ranks_irr = utils.compute_ranks(txt_irr, img_irr)\n",
    "ranks_all = utils.compute_ranks(txt_all, img_all)\n",
    "ranks_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare baseline with single-discourse model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which single-discourse model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1512, 1) (1512, 1) (1512, 40) (1512, 1512) (1512,)\n",
      "(1512, 6) (1512, 6) (1512, 40) (1512, 1512) (1512,)\n"
     ]
    }
   ],
   "source": [
    "# relation = 'Visible'\n",
    "# probs = probs_vis\n",
    "# labels = labels_vis\n",
    "# attns = attns_vis\n",
    "# ranks = ranks_vis\n",
    "# relation_idx = 0\n",
    "\n",
    "# relation = 'Subjective'\n",
    "# probs = probs_sub\n",
    "# labels = labels_sub\n",
    "# attns = attns_sub\n",
    "# ranks = ranks_sub\n",
    "# relation_idx = 1\n",
    "\n",
    "# relation = 'Action'\n",
    "# probs = probs_act\n",
    "# labels = labels_act\n",
    "# attns = attns_act\n",
    "# ranks = ranks_act\n",
    "# relation_idx = 2\n",
    "\n",
    "# relation = 'Story'\n",
    "# probs = probs_sto\n",
    "# labels = labels_sto\n",
    "# attns = attns_sto\n",
    "# ranks = ranks_sto\n",
    "# relation_idx = 3\n",
    "\n",
    "relation = 'Meta'\n",
    "probs = probs_met\n",
    "labels = labels_met\n",
    "attns = attns_met\n",
    "ranks = ranks_met\n",
    "relation_idx = 4\n",
    "\n",
    "\n",
    "# relation = 'Irrelavant'\n",
    "# probs = probs_irr\n",
    "# labels = labels_irr\n",
    "# attns = attns_irr\n",
    "# ranks = ranks_irr\n",
    "# relation_idx = 5\n",
    "\n",
    "\n",
    "def get_pos(ranks):\n",
    "    out = []\n",
    "    for ii, rank in enumerate(ranks):\n",
    "        pos = rank.tolist().index(ii)\n",
    "        out.append(pos)\n",
    "    return np.asarray(out)\n",
    "\n",
    "positions = get_pos(ranks)\n",
    "positions_all = get_pos(ranks_all)\n",
    "\n",
    "print(probs.shape, labels.shape, attns.shape, ranks.shape, positions.shape)\n",
    "print(probs_all.shape, labels_all.shape, attns_all.shape, ranks_all.shape, positions_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discourse is True or False?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003,)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 1\n",
    "t_indices = np.where(labels==T)[0]\n",
    "t_positions = positions[t_indices]\n",
    "tmp = np.argsort(t_positions, 0).squeeze()\n",
    "t_indices = t_indices[tmp]\n",
    "t_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5348"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "img2id = json.load(open('./../data/conceptual/img2idxmap.json', 'r'))\n",
    "len(img2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgheader = 'url'\n",
    "capheader = 'caption'\n",
    "img_dir = '../data/conceptual/images/'\n",
    "save_dir = f'outputs/clue/{relation}={T}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which sample to show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/clue/Visible=1/coherence_win\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = os.path.join(save_dir, 'coherence_win')\n",
    "# save_dir = os.path.join(save_dir, 'coherence_lose')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "good_indices = []\n",
    "for a,b,idx in zip(positions[t_indices], positions_all[t_indices], t_indices):\n",
    "    if a<5 and b-a >= 5:\n",
    "#     if b-a >0:\n",
    "#     if b<5 and a-b >= 5:\n",
    "        good_indices.append(idx)\n",
    "    \n",
    "\n",
    "print(save_dir)\n",
    "len(good_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 2300.14it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(save_dir, 'captions.txt'), 'w') as f:\n",
    "    for idx in tqdm(good_indices[:20]):\n",
    "        rcp = test_set.recipes.iloc[idx]\n",
    "        cap = rcp[capheader]\n",
    "        f.write(f'{idx:>5d} ' + cap + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:53<00:00,  2.67s/it]\n"
     ]
    }
   ],
   "source": [
    "my_trans = transforms.Compose([\n",
    "    transforms.Resize(512),\n",
    "    transforms.CenterCrop(512)\n",
    "])\n",
    "\n",
    "\n",
    "def show_img(rcp, title=None):\n",
    "    img_name = img2id[rcp[imgheader]]\n",
    "    img_path = './../data/conceptual/images/{}.jpg'.format(img_name)\n",
    "    img = my_trans(Image.open(img_path))\n",
    "#     _ = plt.imshow(np.asarray(img))\n",
    "#     plt.axis('off')\n",
    "    if title:\n",
    "        img.save(title, 'JPEG')\n",
    "\n",
    "# idx = 96\n",
    "\n",
    "for idx in tqdm(good_indices[:20]):\n",
    "    sub_dir = os.path.join(save_dir, str(idx))\n",
    "    os.makedirs(sub_dir, exist_ok=True)\n",
    "    f = open(os.path.join(sub_dir, 'attentions.txt'), 'w')\n",
    "\n",
    "    rcp = test_set.recipes.iloc[idx]\n",
    "    cap = rcp[capheader]\n",
    "    cap = utils.clean_caption(cap)\n",
    "    words = re.split(r'\\\\n| ', cap)[:args.max_len]\n",
    "#     print(cap)\n",
    "#     print(words)\n",
    "\n",
    "#     print(f'\\n==> {relation} == {T}')\n",
    "#     print(probs[idx])\n",
    "    f.write(f'Model: {relation} is used\\n')\n",
    "    f.write(f'prob = {probs[idx][0]:.4f}\\n')\n",
    "    for w, attn in zip(words, attns[idx][:len(words)]):\n",
    "        line = f'{w:>20s} = {attn:<.4f}'\n",
    "#         print(line)\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "\n",
    "#     print('\\n==> no relation')\n",
    "#     print(probs_all[idx][relation_idx])\n",
    "    f.write(f'\\nModel: no relation is used\\n')\n",
    "    f.write(f'prob = {probs_all[idx][relation_idx]:.4f}\\n')\n",
    "    for w, attn in zip(words, attns_all[idx][:len(words)]):\n",
    "        line = f'{w:>20s} = {attn:<.4f}'\n",
    "#         print(line)\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "\n",
    "    show_img(rcp, title=os.path.join(sub_dir, f'real.jpg'))\n",
    "\n",
    "    # ranks\n",
    "    rank = ranks[idx]\n",
    "#     fig = plt.figure(figsize=(12,6))\n",
    "    pos = rank.tolist().index(idx)\n",
    "    line = f'{relation} == {T}. Top 5 retrieved images, while true image is at pos={pos}'\n",
    "#     fig.suptitle(line, y=0.7)\n",
    "    f.write(line+'\\n')\n",
    "    i = 0\n",
    "    for idx_ in rank[:5]:\n",
    "#         plt.subplot(151+i)\n",
    "        show_img(test_set.recipes.iloc[idx_], title=os.path.join(sub_dir, f'{relation}_top{i}.jpg'))\n",
    "        i+=1\n",
    "\n",
    "\n",
    "    rank_all = ranks_all[idx]\n",
    "#     fig = plt.figure(figsize=(12,6))\n",
    "    pos = rank_all.tolist().index(idx)\n",
    "    line = f'No Relation. Top 5 retrieved images, while true image is at pos={pos}'\n",
    "#     fig.suptitle(line, y=0.7)\n",
    "    f.write(line+'\\n')\n",
    "    i = 0\n",
    "    for idx_ in rank_all[:5]:\n",
    "#         plt.subplot(151+i)\n",
    "        show_img(test_set.recipes.iloc[idx_], title=os.path.join(sub_dir, f'no_relation_top{i}.jpg'))\n",
    "        i+=1\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate samples for human evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which single-discourse model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1512, 1) (1512, 1) (1512, 40) (1512, 1512) (1512,)\n",
      "(1512, 6) (1512, 6) (1512, 40) (1512, 1512) (1512,)\n"
     ]
    }
   ],
   "source": [
    "# relation = 'Visible'\n",
    "# probs = probs_vis\n",
    "# labels = labels_vis\n",
    "# attns = attns_vis\n",
    "# ranks = ranks_vis\n",
    "# relation_idx = 0\n",
    "\n",
    "# relation = 'Subjective'\n",
    "# probs = probs_sub\n",
    "# labels = labels_sub\n",
    "# attns = attns_sub\n",
    "# ranks = ranks_sub\n",
    "# relation_idx = 1\n",
    "\n",
    "# relation = 'Action'\n",
    "# probs = probs_act\n",
    "# labels = labels_act\n",
    "# attns = attns_act\n",
    "# ranks = ranks_act\n",
    "# relation_idx = 2\n",
    "\n",
    "# relation = 'Story'\n",
    "# probs = probs_sto\n",
    "# labels = labels_sto\n",
    "# attns = attns_sto\n",
    "# ranks = ranks_sto\n",
    "# relation_idx = 3\n",
    "\n",
    "# relation = 'Meta'\n",
    "# probs = probs_met\n",
    "# labels = labels_met\n",
    "# attns = attns_met\n",
    "# ranks = ranks_met\n",
    "# relation_idx = 4\n",
    "\n",
    "\n",
    "relation = 'Irrelavant'\n",
    "probs = probs_irr\n",
    "labels = labels_irr\n",
    "attns = attns_irr\n",
    "ranks = ranks_irr\n",
    "relation_idx = 5\n",
    "\n",
    "def get_pos(ranks):\n",
    "    out = []\n",
    "    for ii, rank in enumerate(ranks):\n",
    "        pos = rank.tolist().index(ii)\n",
    "        out.append(pos)\n",
    "    return np.asarray(out)\n",
    "\n",
    "positions = get_pos(ranks)\n",
    "positions_all = get_pos(ranks_all)\n",
    "\n",
    "print(probs.shape, labels.shape, attns.shape, ranks.shape, positions.shape)\n",
    "print(probs_all.shape, labels_all.shape, attns_all.shape, ranks_all.shape, positions_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 1\n",
    "t_indices = np.where(labels==T)[0]\n",
    "t_positions = positions[t_indices]\n",
    "tmp = np.argsort(t_positions, 0).squeeze()\n",
    "t_indices = t_indices[tmp]\n",
    "t_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/clue/Irrelavant=1/human_evaluation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = f'outputs/clue/{relation}={T}/human_evaluation'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "good_indices = []\n",
    "for a,b,idx in zip(positions[t_indices], positions_all[t_indices], t_indices):\n",
    "    if b-a >0:\n",
    "        good_indices.append(idx)\n",
    "\n",
    "print(save_dir)\n",
    "len(good_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5348"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "img2id = json.load(open('./../data/conceptual/img2idxmap.json', 'r'))\n",
    "len(img2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgheader = 'url'\n",
    "capheader = 'caption'\n",
    "img_dir = '../data/conceptual/images/'\n",
    "save_dir = f'outputs/clue/{relation}={T}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:00<00:00, 3225.48it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(save_dir, 'captions.txt'), 'w') as f:\n",
    "    for idx in tqdm(good_indices[:100]):\n",
    "        rcp = test_set.recipes.iloc[idx]\n",
    "        cap = rcp[capheader]\n",
    "        f.write(f'{idx:>5d} ' + cap + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:32<00:00,  2.07it/s]\n"
     ]
    }
   ],
   "source": [
    "my_trans = transforms.Compose([\n",
    "    transforms.Resize(512),\n",
    "    transforms.CenterCrop(512)\n",
    "])\n",
    "\n",
    "\n",
    "def show_img(rcp, title=None):\n",
    "    img_name = img2id[rcp[imgheader]]\n",
    "    img_path = './../data/conceptual/images/{}.jpg'.format(img_name)\n",
    "    img = my_trans(Image.open(img_path))\n",
    "    if title:\n",
    "        img.save(title, 'JPEG')\n",
    "\n",
    "\n",
    "for idx in tqdm(good_indices[:100]):\n",
    "    rcp = test_set.recipes.iloc[idx]\n",
    "\n",
    "    rank = ranks[idx]\n",
    "    for idx_ in rank[:1]:\n",
    "        show_img(test_set.recipes.iloc[idx_], title=os.path.join(save_dir, f'{idx}_cohaware.jpg'))\n",
    "\n",
    "\n",
    "    rank_all = ranks_all[idx]\n",
    "    for idx_ in rank_all[:1]:\n",
    "        show_img(test_set.recipes.iloc[idx_], title=os.path.join(save_dir, f'{idx}_cohagnostic.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
