{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pprint on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import utils\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "import json\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from types import SimpleNamespace\n",
    "from networks import TextEncoder, ImageEncoder, DiscourseClassifier\n",
    "from datasets import CoherenceDataset, val_transform\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import transforms\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, with_attention=2):\n",
    "    text_encoder = TextEncoder(\n",
    "        emb_dim=args.word2vec_dim,\n",
    "        hid_dim=args.rnn_hid_dim,\n",
    "        z_dim=args.feature_dim,\n",
    "        max_len = args.max_len,\n",
    "        word2vec_file=f'models/word2vec_{args.data_source}.bin',\n",
    "        with_attention=with_attention).to(device)\n",
    "    image_encoder = ImageEncoder(\n",
    "        z_dim=args.feature_dim).to(device)\n",
    "    discourse_class = DiscourseClassifier(\n",
    "        len(relations), args.feature_dim).to(device)\n",
    "\n",
    "    ckpt = torch.load(path)\n",
    "    text_encoder.load_state_dict(ckpt['text_encoder'])\n",
    "    image_encoder.load_state_dict(ckpt['image_encoder'])\n",
    "    discourse_class.load_state_dict(ckpt['discourse_class'])\n",
    "    return text_encoder, image_encoder, discourse_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(test_loader, text_encoder, image_encoder, discourse_class, valid_questions):\n",
    "    txt_feats = []\n",
    "    img_feats = []\n",
    "    probs = []\n",
    "    labels = []\n",
    "    attns = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        for i in range(len(batch)):\n",
    "            batch[i] = batch[i].to(device)\n",
    "        txt, txt_len, img, target = batch\n",
    "        with torch.no_grad():\n",
    "            txt_feat, attn = text_encoder(txt.long(), txt_len)\n",
    "            img_feat = image_encoder(img)\n",
    "            prob = torch.sigmoid(discourse_class(txt_feat, img_feat))[:,valid_questions]\n",
    "            txt_feats.append(txt_feat.detach().cpu())\n",
    "            img_feats.append(img_feat.detach().cpu())\n",
    "            probs.append(prob.detach().cpu())\n",
    "            if text_encoder.with_attention:\n",
    "                attns.append(attn.detach().cpu())\n",
    "            labels.append(target[:,valid_questions].detach().cpu())\n",
    "\n",
    "    txt_feats = torch.cat(txt_feats, dim=0).numpy()\n",
    "    img_feats = torch.cat(img_feats, dim=0).numpy()\n",
    "    probs = torch.cat(probs, dim=0).numpy()\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    if text_encoder.with_attention:\n",
    "        attns = torch.cat(attns, dim=0).numpy()\n",
    "    return probs, labels, attns, txt_feats, img_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(rcps, imgs, retrieved_type='image', retrieved_range=1000, n_repeat=30):\n",
    "    N = retrieved_range\n",
    "    data_size = imgs.shape[0]\n",
    "    idxs = range(N)\n",
    "    glob_rank = []\n",
    "    glob_recall = {1: 0.0, 5: 0.0, 10: 0.0}\n",
    "    # average over 20 sets\n",
    "    for i in range(n_repeat):\n",
    "        ids_sub = np.random.choice(data_size, N, replace=False)\n",
    "        imgs_sub = imgs[ids_sub, :]\n",
    "        rcps_sub = rcps[ids_sub, :]\n",
    "        imgs_sub = imgs_sub / np.linalg.norm(imgs_sub, axis=1)[:, None]\n",
    "        rcps_sub = rcps_sub / np.linalg.norm(rcps_sub, axis=1)[:, None]\n",
    "        if retrieved_type == 'recipe':\n",
    "            sims = np.dot(imgs_sub, rcps_sub.T)  # [N, N]\n",
    "        else:\n",
    "            sims = np.dot(rcps_sub, imgs_sub.T)\n",
    "        med_rank = []\n",
    "        recall = {1: 0.0, 5: 0.0, 10: 0.0}\n",
    "        # loop through the N similarities for images\n",
    "        for ii in idxs:\n",
    "            # get a column of similarities for image ii\n",
    "            sim = sims[ii, :]\n",
    "            # sort indices in descending order\n",
    "            sorting = np.argsort(sim)[::-1].tolist()\n",
    "            # find where the index of the pair sample ended up in the sorting\n",
    "            pos = sorting.index(ii)\n",
    "            if (pos + 1) == 1:\n",
    "                recall[1] += 1\n",
    "            if (pos + 1) <= 5:\n",
    "                recall[5] += 1\n",
    "            if (pos + 1) <= 10:\n",
    "                recall[10] += 1\n",
    "            # store the position\n",
    "            med_rank.append(pos+1)\n",
    "\n",
    "        for i in recall.keys():\n",
    "            recall[i] = recall[i]/N\n",
    "        med = np.median(med_rank)\n",
    "        for i in recall.keys():\n",
    "            glob_recall[i] += recall[i]\n",
    "        glob_rank.append(med)\n",
    "\n",
    "    for i in glob_recall.keys():\n",
    "        glob_recall[i] = glob_recall[i]/10\n",
    "    return np.asarray(glob_rank), glob_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxplot the performances of different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, name, path, questions, with_attention):\n",
    "        self.name = name\n",
    "        self.path = path\n",
    "        self.questions = questions\n",
    "        self.with_attention = with_attention\n",
    "        self.medRs = None\n",
    "        self.recalls = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "args = {\n",
    "    'data_source': 'clue',\n",
    "    'img_dir': '../data/conceptual/images/',\n",
    "    'word2vec_dim': 300,\n",
    "    'rnn_hid_dim': 300,\n",
    "    'feature_dim': 1024,\n",
    "    'max_len': 40,\n",
    "    'dataset_q': 0,\n",
    "    'with_attention': 2,\n",
    "    'batch_size': 64,\n",
    "    'workers': 4\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "relations = relations = ['Visible', 'Subjective', 'Action', 'Story', 'Meta', 'Irrelevant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size = 5612\n",
      "test data: 1512 24\n"
     ]
    }
   ],
   "source": [
    "test_set = CoherenceDataset(\n",
    "            part='test',\n",
    "            datasource=args.data_source,\n",
    "            word2vec_file=f'models/word2vec_{args.data_source}.bin',\n",
    "            max_len=args.max_len,\n",
    "            dataset_q=args.dataset_q,  # experimental things, ignore it for now\n",
    "            transform=val_transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "            test_set, batch_size=args.batch_size, shuffle=False,\n",
    "            num_workers=args.workers, pin_memory=True,\n",
    "            drop_last=False)\n",
    "\n",
    "print('test data:', len(test_set), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = []\n",
    "# Base\n",
    "path = 'runs/samples6047_retrieval=1.00_classification=0.00_reweight=1000.00_weightDecay=0.0_withAttention=0_question=0,1,2,3,4,5_maxLen=40/e14.ckpt'\n",
    "valid_questions = torch.tensor([0,1,2,3,4,5], dtype=torch.long)\n",
    "with_attention = 0\n",
    "configs.append(Config('Base', path, valid_questions, with_attention))\n",
    "\n",
    "# CMCA\n",
    "path = 'runs/samples6047_retrieval=1.00_classification=0.00_reweight=1000.00_weightDecay=0.0_withAttention=2_question=0,1,2,3,4,5_maxLen=40/e14.ckpt'\n",
    "valid_questions = torch.tensor([0,1,2,3,4,5], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config('CMCA', path, valid_questions, with_attention))\n",
    "\n",
    "# CMCM-NoAttn\n",
    "path = 'runs/samples6047_retrieval=1.00_classification=0.00_reweight=1000.00_weightDecay=0.0_withAttention=0_question=0,1,2,3,4,5_maxLen=40/e14.ckpt'\n",
    "valid_questions = torch.tensor([0,1,2,3,4,5], dtype=torch.long)\n",
    "with_attention = 0\n",
    "configs.append(Config('CMCM-NoAttn', path, valid_questions, with_attention))\n",
    "\n",
    "# CMCM\n",
    "path = 'runs/samples6047_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=0,1,2,3,4,5_maxLen=40/e14.ckpt'\n",
    "valid_questions = torch.tensor([0,1,2,3,4,5], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config('CMCM', path, valid_questions, with_attention))\n",
    "\n",
    "# Visible\n",
    "path = 'runs/samples6047_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=0_maxLen=40/e14.ckpt'\n",
    "valid_questions = torch.tensor([0], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config('CMCM_{Visible}', path, valid_questions, with_attention))\n",
    "\n",
    "# Subjective\n",
    "path = 'runs/samples6047_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=1_maxLen=40/e14.ckpt'\n",
    "valid_questions = torch.tensor([1], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config('CMCM_{Subjective}', path, valid_questions, with_attention))\n",
    "\n",
    "# Action\n",
    "path = 'runs/samples6047_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=2_maxLen=40/e14.ckpt'\n",
    "valid_questions = torch.tensor([2], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config('CMCM_{Action}', path, valid_questions, with_attention))\n",
    "\n",
    "# Story\n",
    "path = 'runs/samples6047_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=3_maxLen=40/e14.ckpt'\n",
    "valid_questions = torch.tensor([3], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config('CMCM_{Story}', path, valid_questions, with_attention))\n",
    "\n",
    "# Meta\n",
    "path = 'runs/samples6047_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=4_maxLen=40/e14.ckpt'\n",
    "valid_questions = torch.tensor([4], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config('CMCM_{Meta}', path, valid_questions, with_attention))\n",
    "\n",
    "# Irrelavant\n",
    "path = 'runs/samples6047_retrieval=1.00_classification=0.10_reweight=1000.00_weightDecay=0.0_withAttention=2_question=5_maxLen=40/e14.ckpt'\n",
    "valid_questions = torch.tensor([5], dtype=torch.long)\n",
    "with_attention = 2\n",
    "configs.append(Config('CMCM_{Irrelavant}', path, valid_questions, with_attention))\n",
    "\n",
    "len(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Config object at 0x7f0c934072e0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:49<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n",
      "<__main__.Config object at 0x7f0cb97b7be0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:50<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n",
      "<__main__.Config object at 0x7f0c2d548520>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:48<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n",
      "<__main__.Config object at 0x7f0c2d548430>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:48<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n",
      "<__main__.Config object at 0x7f0c2d548460>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:50<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n",
      "<__main__.Config object at 0x7f0c2d548400>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:49<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n",
      "<__main__.Config object at 0x7f0c2d548490>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:50<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n",
      "<__main__.Config object at 0x7f0c2d5484c0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:51<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n",
      "<__main__.Config object at 0x7f0c2d548550>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:48<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n",
      "<__main__.Config object at 0x7f0c2d548580>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:49<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_range = 500\n"
     ]
    }
   ],
   "source": [
    "for config in configs:\n",
    "    print(config)\n",
    "    txt_encoder, img_encoder, classifier = load_model(config.path, with_attention=config.with_attention)\n",
    "    probs_test, labels_test, attns_test, txt_test, img_test = generate_output(test_loader, txt_encoder, img_encoder, classifier, config.questions)\n",
    "    probs_test.shape\n",
    "    retrieved_range = min(txt_test.shape[0], 500)\n",
    "    print('retrieved_range =', retrieved_range)\n",
    "    medRs, recalls = rank(\n",
    "        txt_test, \n",
    "        img_test, \n",
    "        retrieved_type='image', \n",
    "        retrieved_range=retrieved_range,\n",
    "        n_repeat=50)\n",
    "    config.medRs = medRs\n",
    "    config.recalls = recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Base\n",
      "24.32 2.721690651047617\n",
      "1 0.5783999999999999\n",
      "5 1.3601999999999994\n",
      "10 1.8210000000000002\n",
      "\n",
      " CMCA\n",
      "21.19 2.0539961051569695\n",
      "1 0.6521999999999999\n",
      "5 1.4821999999999997\n",
      "10 1.9376000000000002\n",
      "\n",
      " CMCM-NoAttn\n",
      "24.67 2.7433738352619756\n",
      "1 0.5808\n",
      "5 1.389\n",
      "10 1.828\n",
      "\n",
      " CMCM\n",
      "21.04 2.1280037593951757\n",
      "1 0.6472000000000001\n",
      "5 1.4409999999999996\n",
      "10 1.9308000000000003\n",
      "\n",
      " CMCM_{Visible}\n",
      "22.18 3.2011872797448135\n",
      "1 0.6360000000000001\n",
      "5 1.4991999999999996\n",
      "10 1.9889999999999997\n",
      "\n",
      " CMCM_{Subjective}\n",
      "27.96 3.1604430069216565\n",
      "1 0.5299999999999999\n",
      "5 1.299\n",
      "10 1.7575999999999994\n",
      "\n",
      " CMCM_{Action}\n",
      "22.49 2.7613221470882388\n",
      "1 0.6411999999999999\n",
      "5 1.4710000000000003\n",
      "10 1.9116000000000004\n",
      "\n",
      " CMCM_{Story}\n",
      "20.66 2.093895890439637\n",
      "1 0.6234\n",
      "5 1.4506\n",
      "10 1.9300000000000002\n",
      "\n",
      " CMCM_{Meta}\n",
      "21.37 2.6245190035509363\n",
      "1 0.6454\n",
      "5 1.4602000000000004\n",
      "10 1.9242000000000004\n",
      "\n",
      " CMCM_{Irrelavant}\n",
      "22.2 2.6362852652928135\n",
      "1 0.6391999999999998\n",
      "5 1.4703999999999993\n",
      "10 1.9314000000000004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1152 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAFpCAYAAABuwbWeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df3TldX3n8efbmWFmwCloHSk/HXStBgMMbg51S3Q3iBQGq1ZrNW0tbbPgtJjqgrszktOD2g3LrBXdom3KECpaGvEUURdmLeiOa7Ot6EAHBOMPyqEVh8roLojA4gy+94/vd8idLze5N8lN7k3yfJxzT+73Z96fT+795nW/93O/NzITSZIkSZOe1e4CJEmSpE5jSJYkSZIqDMmSJElShSFZkiRJqjAkS5IkSRUr211APc973vNyw4YN7S5DkiRJS9jtt9/+g8xcX29ZR4bkDRs2sGvXrnaXIUmSpCUsIv5pqmUOt5AkSZIqDMmSJElShSFZkiRJqjAkS5IkSRWGZEmSJKnCkCxJkiRVGJIlSZKkCkOyJEmSVGFIliRJkioMyZIkSVJFw5AcEWsi4qsRcWdE3BMR7yvnvzcivhcRu8vbpim2PzsivhUR90bE1lY3QJLUfoODg6xZs4aIYM2aNQwODra7JEmak2bOJD8JnJGZpwAbgbMj4hXlsg9l5sbytqO6YUSsAD4KnAOcCPRHxIktql2S1AEGBwcZGRnhsssu47HHHuOyyy5jZGTEoCxpUWsYkrPw43JyVXnLJvd/GnBvZt6XmT8BPgm8flaVSpI60vbt29m2bRsXXXQRhx56KBdddBHbtm1j+/bt7S5NkmatqTHJEbEiInYDDwG3ZuZt5aJ3RMRdEXFNRDynzqbHAN+tmX6gnFfvd1wQEbsiYtfevXtn0ARJUjs9+eSTbN68+aB5mzdv5sknn2xTRZI0d02F5Mx8KjM3AscCp0VEN/BnwIsohmA8CHywzqZRb3dT/I6rMrMnM3vWr1/fVPGSpPZbvXo1IyMjB80bGRlh9erVbapIkuZuRle3yMyHgS8BZ2fm98vw/FNgO8XQiqoHgONqpo8F9syyVklSBzr//PPZsmULV1xxBY8//jhXXHEFW7Zs4fzzz293aZI0aysbrRAR64F9mflwRKwFzgS2RcRRmflgudqvAHfX2fxrwIsj4gTge8BbgV9vTemSpE5w5ZVXAnDJJZdw8cUXs3r1ajZv3vz0fElajCJz+s/gRcTJwLXACoozz5/KzPdHxCcohlokcD/w9sx8MCKOBq7OzE3l9puAD5fbX5OZw42K6unpyV27ds2+VZIkSVIDEXF7ZvbUXdYoJLeDIVmSJEnzbbqQ7DfuSZIkSRWGZEmSJKnCkCxJkiRVGJIlSZKkCkOyJEmSVGFIliRJkioMyZIkSVKFIVmSJEmqMCRLkiRJFYZkSZIkqcKQLEmSJFUYkiVJkqQKQ7IkSZJUYUiWJEmSKgzJkiRJUoUhWZIkSaowJEuSJEkVK9tdgCRpcYiIluwnM1uyH0maT55JliQ1JTMb3l6w5aaG60jSYmBIliRJkioMyZIkSVKFIVmSJEmqMCRLTRgbG6O7u5sVK1bQ3d3N2NhYu0uSJEnzyKtbSA2MjY0xNDTE6Ogovb29jI+PMzAwAEB/f3+bq5MkSfPBM8lSA8PDw4yOjtLX18eqVavo6+tjdHSU4eHhdpcmSZLmiSFZamBiYoLe3t6D5vX29jIxMdGmiiRJ0nwzJEsNdHV1MT4+ftC88fFxurq62lSRJEmab4ZkqYGhoSEGBgbYuXMn+/btY+fOnQwMDDA0NNTu0iRJ0jzxg3tSAwc+nDc4OMjExARdXV0MDw/7oT1JkpYwQ7LUhP7+fkOxJEnLSMOQHBFrgC8Dq8v1/zozL42IDwC/DPwE+EfgdzLz4Trb3w88CjwF7M/MntaVL0mSJLVeM2OSnwTOyMxTgI3A2RHxCuBWoDszTwa+Dbxnmn30ZeZGA7IkSZIWg4YhOQs/LidXlbfMzFsyc385/yvAsfNUoyRJkrSgmrq6RUSsiIjdwEPArZl5W2WV3wX+xxSbJ3BLRNweERdM8zsuiIhdEbFr7969zZQlSZIkzYumQnJmPpWZGynOFp8WEd0HlkXEELAfuG6KzU/PzJcD5wAXRsSrpvgdV2VmT2b2rF+/fkaNkCRJklppRtdJLj+Y9yXgbICIOA94LfAbmZlTbLOn/PkQcCNw2hzqlSRJkuZdw5AcEesj4ojy/lrgTOCbEXE2sAV4XWY+PsW2h0XEugP3gbOAu1tVvCRJkjQfmrlO8lHAtRGxgiJUfyozb4qIeykuC3drRAB8JTM3R8TRwNWZuQk4ErixXL4S+KvM/Px8NESSJElqlWaubnFXZp6amSdnZndmvr+c/68y87jy0m4bM3NzOX9PGZDJzPsy85Ty9rLMHJ7f5kjzY2xsjO7ublasWEF3dzdjY2PtLkmSJM0jv3FPamBsbIyhoSFGR0fp7e1lfHycgYEBAL+FT5KkJWpGH9yTlqPh4WFGR0fp6+tj1apV9PX1MTo6yvCwb4xIkrRUGZKlBiYmJujt7T1oXm9vLxMTE22qSJIkzTdDstRAV1cX4+PjB80bHx+nq6urTRVJkqT5ZkiWGhgaGmJgYICdO3eyb98+du7cycDAAENDQ+0uTZIkzRM/uCc1cODDeYODg0xMTNDV1cXw8LAf2pMkaQkzJEtN6O/vNxRLkrSMONxCkiRJqjAkS5IkSRWGZEmSJKnCkCxJkiRVGJIlSZKkCkOyJEmSVGFIliRJkioMyZIkSVKFIVmSJEmqMCRLkiRJFcv2a6kjYs77yMwWVCJJWmxa8T8E/D8idbJleyY5M6e9vWDLTQ3XkSQtT43+P/h/RFr8lm1IliRJkqZiSJYkSZIqDMmSJElShSFZkiRJqjAkS5IkSRWGZEmSJKli2V4nWZKa5XXVJWn58UyyJDXg9XAlafkxJEuSJEkVhmRJkiSpwpAsSZIkVTQMyRGxJiK+GhF3RsQ9EfG+cv5zI+LWiPhO+fM5U2x/dkR8KyLujYitrW6AJEmS1GrNnEl+EjgjM08BNgJnR8QrgK3AFzPzxcAXy+mDRMQK4KPAOcCJQH9EnNiq4iVJkqT50DAkZ+HH5eSq8pbA64Fry/nXAm+os/lpwL2ZeV9m/gT4ZLmdJEmS1LGaGpMcESsiYjfwEHBrZt4GHJmZDwKUP59fZ9NjgO/WTD9Qzqv3Oy6IiF0RsWvv3r0zaYPUMhEx55skSVr8mgrJmflUZm4EjgVOi4juJvdfLzHUvWBoZl6VmT2Z2bN+/fomdy+1ltfDlSRJMMOrW2Tmw8CXgLOB70fEUQDlz4fqbPIAcFzN9LHAnllVKkmSJC2QZq5usT4ijijvrwXOBL4JfA44r1ztPOCzdTb/GvDiiDghIg4B3lpuJ0mSJHWslU2scxRwbXmlimcBn8rMmyLi74FPRcQA8M/AmwEi4mjg6szclJn7I+IdwN8AK4BrMvOeeWmJJEmS1CINQ3Jm3gWcWmf+D4FX15m/B9hUM70D2DG3MiVJkqSF4zfuSZIkSRWGZEmSJKmimTHJkqRl4JT33cIjT+yb8342bL15TtsfvnYVd1561pzrkNQ+rfregHZeWtWQLEkC4JEn9nH/5ee2u4w5h2xJ7ddMuN2w9eaOOOZMxeEWkiRJUoUhWZIkSaowJEuSJEkVhmRJkiSpwpAsSZIkVRiSJUmSpApDsiRJklRhSJYkSZIqDMmSJElShSFZkiRJqjAkS5IkSRWGZEmSJKnCkCxJkiRVGJIlSZKkCkOyJEmSVGFIliRJkioMyZIkSVKFIVmSJEmqMCRrSmNjY3R3d7NixQq6u7sZGxtrd0mSJEkLYmW7C1BnGhsbY2hoiNHRUXp7exkfH2dgYACA/v7+NlcnSZI0vzyTrLqGh4cZHR2lr6+PVatW0dfXx+joKMPDw+0uTZIkad55Jll1TUxM0Nvbe9C83t5eJiYm2lSRJEmdLSJasp/MbMl+NDeeSVZdXV1djI+PHzRvfHycrq6uNlUkSVJny8yGtxdsuanhOuoMhmTVNTQ0xMDAADt37mTfvn3s3LmTgYEBhoaG2l2aJEnSvHO4heo68OG8wcFBJiYm6OrqYnh42A/tSZKkZcGQrCn19/cbiiVJ0rLUMCRHxHHAx4GfA34KXJWZ/y0irgdeUq52BPBwZm6ss/39wKPAU8D+zOxpUe2SJEnSvGjmTPJ+4OLMvCMi1gG3R8StmfmWAytExAeBR6bZR19m/mCOtUqSJEkLomFIzswHgQfL+49GxARwDPANgCiud/JrwBnzWKckSZK0YGZ0dYuI2ACcCtxWM/uVwPcz8ztTbJbALRFxe0RcMM2+L4iIXRGxa+/evTMpS5IkSWqppkNyRDwbuAF4V2b+qGZRPzA2zaanZ+bLgXOACyPiVfVWysyrMrMnM3vWr1/fbFmSJElSyzUVkiNiFUVAvi4zP10zfyXwRuD6qbbNzD3lz4eAG4HT5lKwJEmSNN8ahuRyzPEoMJGZV1QWnwl8MzMfmGLbw8oP+xERhwFnAXfPrWRJkiRpfjVzdYvTgbcBX4+I3eW8SzJzB/BWKkMtIuJo4OrM3AQcCdxYfpf5SuCvMvPzrSpektQ667q2ctK1W9tdBuu6AM5tdxmSlrlmrm4xDsQUy367zrw9wKby/n3AKXMrUZK0EB6duJz7L29/ON2w9eZ2lyBJM7u6hSRJkrQcGJIlSZKkCkOyJEmSVGFIliRJkioMyZIkSVKFIVmSJEmqMCRLkiRJFYZkSZIkqcKQLEmSJFUYkiVJkqQKQ7IkSZJUYUiWJEmSKgzJkiRJUoUhWZIkSaowJEuSJEkVhmRJMzI2NkZ3dzcrVqygu7ubsbGxdpckSVLLrWx3AZIWj7GxMYaGhhgdHaW3t5fx8XEGBgYA6O/vb3N1kiS1jmeSJTVteHiY0dFR+vr6WLVqFX19fYyOjjI8PNzu0iRJaqkleSb5lPfdwiNP7JvzfjZsvXlO2x++dhV3XnrWnOuQOsXExAS9vb0Hzevt7WViYqJNFanV5nrca4XD165qdwmStDRD8iNP7OP+y89tdxkd8c9Gk3zxNHddXV2Mj4/T19f39Lzx8XG6urraWJVapRXHzQ1bb+6I468kzdWSDMlSPb54mruhoSEGBgaeMSbZ4RaSpKXGkCypaQc+nDc4OMjExARdXV0MDw/7oT1J0pJjSJY0I/39/YZiSdKS59UtJEmSpApDsiRJklThcAtJdUXEnPeRmS2oRFIna8WxAjxeqPN4JllSXZk57e0FW25quI6kpa/RccDjhRYrQ7IkSZJUYUiWJEmSKgzJkiRJUkXDkBwRx0XEzoiYiIh7IuKd5fz3RsT3ImJ3eds0xfZnR8S3IuLeiNja6gZIkiRJrdbM1S32Axdn5h0RsQ64PSJuLZd9KDP/eKoNI2IF8FHgNcADwNci4nOZ+Y25Fi5JkiTNl4ZnkjPzwcy8o7z/KDABHNPk/k8D7s3M+zLzJ8AngdfPtlhJkiRpIcxoTHJEbABOBW4rZ70jIu6KiGsi4jl1NjkG+G7N9ANMEbAj4oKI2BURu/bu3TuTsjRHETHnmyRJ0lLSdEiOiGcDNwDvyswfAX8GvAjYCDwIfLDeZnXm1b0YYmZelZk9mdmzfv36ZstSC3h9S0mSpIM19Y17EbGKIiBfl5mfBsjM79cs3w7cVGfTB4DjaqaPBfbMulpJkhbAKe+7hUee2Dfn/WzYevOctj987SruvPSsOdchaeYahuQo3ksfBSYy84qa+Udl5oPl5K8Ad9fZ/GvAiyPiBOB7wFuBX59z1ZIkzaNHntjH/Zef2+4y5hyyJc1eM2eSTwfeBnw9InaX8y4B+iNiI8XwifuBtwNExNHA1Zm5KTP3R8Q7gL8BVgDXZOY9LW6DJEmS1FINQ3JmjlN/bPGOKdbfA2yqmd4x1bqSJElSJ/Ib9yRJkqQKQ7IkSZJUYUiWJEmSKgzJkiRJUkVT10mWpKXK6+FKkuoxJEta1rweriSpHkOylo11XVs56dqt7S6DdV0A7Q9lkiRpaoZkLRuPTlzuGUNJktQUP7gnSZIkVRiSJUmSpIolOdzCsaeSJEnzpxOuDDTfVwVakiHZsaeSJEnzpxOuDDTfOcvhFpIkSVKFIVmSJEmqMCRLkiRJFYZkSZIkqcKQLEmSJFUYkiVJkqQKQ7IkSZJUYUiWJEmSKgzJkiRJUoUhWZIkSaowJEuSJEkVhmRJkiSpwpAsSZIkVaxsdwHSQtqw9eZ2l8Dha1e1uwRJ0iyc8r5beOSJfXPez1z+Fx2+dhV3XnrWnGtQY4ZkLRv3X37unPexYevNLdmPJGnxeeSJfW3/H9AJJ3uWC4dbSJIkSRWGZEmSJKnCkCxJkiRVNAzJEXFcROyMiImIuCci3lnO/0BEfDMi7oqIGyPiiCm2vz8ivh4RuyNiV6sbIEmSJLVaM2eS9wMXZ2YX8Argwog4EbgV6M7Mk4FvA++ZZh99mbkxM3vmXLEkSZI0zxqG5Mx8MDPvKO8/CkwAx2TmLZm5v1ztK8Cx81emJEmStHBmdAm4iNgAnArcVln0u8D1U2yWwC0RkcCfZ+ZVU+z7AuACgOOPP34mZdXVCZdI6YTr4XbCNR3B6zpKkqTFpemQHBHPBm4A3pWZP6qZP0QxJOO6KTY9PTP3RMTzgVsj4puZ+eXqSmV4vgqgp6cnZ9CGZ/B6uJM64ZqO0BkvWiRJkprVVEiOiFUUAfm6zPx0zfzzgNcCr87MusE2M/eUPx+KiBuB04BnhGRJC8d3GCQ1y+OFlquGITkiAhgFJjLzipr5ZwNbgH+bmY9Pse1hwLMy89Hy/lnA+1tSuaRZ8x0GSc3yeKHlqpmrW5wOvA04o7yM2+6I2AR8BFhHMYRid0SMAETE0RGxo9z2SGA8Iu4EvgrcnJmfb30zJEmSpNZpeCY5M8eBqLNoR515B4ZXbCrv3wecMpcCJUmSpIXmN+5JkiRJFYZkSZIkqWJG10mWJC1fxee4m1hv2/TLp7gYUkdZ17WVk67d2u4yWNcF0P4PzUlVnfAcme/nhyFZktSUxRBuW+XRicu9ooM0jU54jsz388PhFpIkSVKFIVmSJEmqcLjFEtcJY4aKOsBxdZIkabEwJC9xnTBmCBxXJ0mSFheHW0iSJEkVhmRJkiSpwpAsSZIkVRiSJUmSpApDsiRJklRhSJYkSZIqvAScpGXNa4lLalYnHC88ViwcQ7KkZc1riUtqViccLzxWLByHW0iSJEkVhmRJkiSpwuEW0jLUCePqijrAsXWSpE5kSJaWoU4YVweOrZMkdS6HW0iSJEkVhmRJkiSpwpAsSZIkVRiSJUmSpApDsiRJklRhSJYkSZIqDMmSJElShSFZkiRJqjAkS5IkSRWGZEmSJKmiYUiOiOMiYmdETETEPRHxznL+cyPi1oj4TvnzOVNsf3ZEfCsi7o2Ira1ugCRJktRqzZxJ3g9cnJldwCuACyPiRGAr8MXMfDHwxXL6IBGxAvgocA5wItBfbitJkiR1rIYhOTMfzMw7yvuPAhPAMcDrgWvL1a4F3lBn89OAezPzvsz8CfDJcjtJkiSpY62cycoRsQE4FbgNODIzH4QiSEfE8+tscgzw3ZrpB4BfmGLfFwAXABx//PEzKUsNbNh6c7tL4PC1q9pdQlMiovE626ZfnpktqmZ++biQpudzpLCuaysnXdv+0ZLrugDObXcZqtHu58h8Pz+aDskR8WzgBuBdmfmjZsIEUG+lugkiM68CrgLo6elZHCljEbj/8rkfUDZsvbkl+1kMFkvAnSsfF9L0fI5MenTi8o5oR7sDmQ62HJ4jTV3dIiJWUQTk6zLz0+Xs70fEUeXyo4CH6mz6AHBczfSxwJ7ZlytJkiTNv2aubhHAKDCRmVfULPoccF55/zzgs3U2/xrw4og4ISIOAd5abidJkiR1rGbOJJ8OvA04IyJ2l7dNwOXAayLiO8Brymki4uiI2AGQmfuBdwB/Q/GBv09l5j3z0A5JkiSpZRqOSc7MceqPLQZ4dZ319wCbaqZ3ADtmW6AkSZK00GZ0dQtJWoo64QNBnXAVA0nSJEOypGVtOXxCW5I0c01d3UKSJElaTgzJkiRJUoUhWZIkSaowJEuSJEkVfnBPkiSpSe2+Go5Xwlk4hmRJkqQmeDWc5cXhFpIkSVKFIVmSJEmqWLbDLSKm+qbtmnW2Tb88M1tUjSRJnavd43DBsbhaeMs2JBtwJUlqzHG4Wq4cbiFJkiRVGJIlSZKkCkOyJEmSVGFIliRJkioMyZIkSVKFIVmSJEmqMCRLkiRJFYZkSZIkqcKQLEmSJFUYkiVJkqQKQ7IkSZJUYUiWJEmSKgzJkiRJUsXKdhcgSdJiExHNrbdt+uWZ2YJqJM0HQ7IkSTNkuJWWPodbSJIkSRWGZEmSJKnCkCxJkiRVNByTHBHXAK8FHsrM7nLe9cBLylWOAB7OzI11tr0feBR4CtifmT0tqluSJEmaN818cO9jwEeAjx+YkZlvOXA/Ij4IPDLN9n2Z+YPZFihJkiQttIYhOTO/HBEb6i2L4ho4vwac0dqyJEmSpPaZ65jkVwLfz8zvTLE8gVsi4vaIuGC6HUXEBRGxKyJ27d27d45lSZIkSbM315DcD4xNs/z0zHw5cA5wYUS8aqoVM/OqzOzJzJ7169fPsSxJkiRp9mYdkiNiJfBG4Pqp1snMPeXPh4AbgdNm+/skSZKkhTKXM8lnAt/MzAfqLYyIwyJi3YH7wFnA3XP4fZIkSdKCaBiSI2IM+HvgJRHxQEQMlIveSmWoRUQcHRE7yskjgfGIuBP4KnBzZn6+daVLkiRJ86OZq1v0TzH/t+vM2wNsKu/fB5wyx/okSZKkBec37kmSJEkVhmRJkiSpoplv3NMSV3wnTIN1tk2/PDNbVI06hY+LSfaFpGY0c6yA5XG8WAp9YUjWkngyqvV8XEyyLyQ1w2PFpKXQFw63kCRJkioMyZIkSVKFIVmSJEmqMCRLkiRJFYZkSZIkqcKQLEmSJFUYkiVJkqQKQ7IkSZJUYUiWJEmSKgzJkiRJUoUhWZIkSaowJEuakcHBQdasWUNEsGbNGgYHB9tdkiRJLWdIltS0wcFBRkZGuOyyy3jssce47LLLGBkZMShLkpYcQ7Kkpm3fvp1t27Zx0UUXceihh3LRRRexbds2tm/f3u7SJElqqcjMdtfwDD09Pblr1652lyGpIiJ47LHHOPTQQ5+e9/jjj3PYYYfRiccSSfMvIlqyH48haoeIuD0ze+ot80yypKatXr2akZGRg+aNjIywevXqNlUkqd0ysyU3qdOsbHcBkhaP888/ny1btgCwefNmRkZG2LJlC5s3b25zZZIktZYhWVLTrrzySgAuueQSLr74YlavXs3mzZufni9J0lLhmGRJkiQtS45JliRJkmbAkCxJkiRVGJIlSZKkCkOyJEmSVGFIliRJkioMyZIkSVKFIVmSJEmqMCRLkiRJFYZkSZIkqaIjv3EvIvYC/9TmMp4H/KDNNXQK+2KSfTHJvphkX0yyLybZF5Psi0n2xaRO6IsXZOb6egs6MiR3gojYNdXXFC439sUk+2KSfTHJvphkX0yyLybZF5Psi0md3hcOt5AkSZIqDMmSJElShSF5ale1u4AOYl9Msi8m2ReT7ItJ9sUk+2KSfTHJvpjU0X3hmGRJkiSpwjPJkiRJUoUhWZIkSaowJEuSJEkVhmRJkiSpYlmF5Ih4e0Q8GBG7I+LeiPhMRBzS7roWWkS8OSJui4i7yn64tOybjIh/W7PeO8p5Z063bWXfH4mIlnxb4nzU2ez2M6zzwD67auZNRMSGJrc/qM8i4tiIeMtU0622WPrZ9jVnubV3tuynZ7JPDrZc+mO5tHM2llVIBk4GLsnMjcDPA93lvGUjIs4DtgBvysyTgY3A4xT9cBfQVa53KDAA7AW+3mDbA/s+Afh3wCERsa5D62y4/SycDOwGzi33uRo4kia+Wn2KPns18PKa1arTLbPI+nnGlnr7qpZbe2fLfnom++Rgy6U/lks7Zy0zl80N+DKwsbz/88A/Aj8D/CrwFeBOYBxYX65zHnA7xR/6b2v2cwLwWWAX8FXgJe1uW5Pt/xngh8ALp+ibrcCHy+mtwCXAvzTatmYfHwfeCnwReEUn1tlo+zk8rt4C7CynXw787/L+S8vl9wBfAJ43XZ8BvWX9/0gRvF9ZmT4BuBH4z8DfAv8CnLkc+tn22V77yT6xP2zngvZRuwtY4AfEDykC7wTwKPCqcv7P1qxzKXAhsA74BnBIOf+I8ucqikDzonJ6E/AX7W5bk+0/D9gxxbK9wAuBzwOHA/9AcYbzC422LZe/DLgDCOAjwECH1jnt9rOs9wfAWoogfHhZwwiwupx3arneFmC4UZ+VtXXXrFed/g7w7vL+G2f7+Fts/Wz7lm97gecsl35qZVsXa5+0ow86uT/mq58WSzvn6W/cVN8tm+EWEXEc8FBmnpyZXRRB+A/Lxb8dEV+NiDuB3wf+H/AURfD5YET0ZObD5bpvoAg3N0TEbuC/lusvBi+jOBt5kLJvfpiZ9wHPB/4TcCXF2fa7ptu2xjDwh1k8+iYohrJ0VJ1Nbl+7/hci4u46t9fX2ecTwK3ALzH5NtMbgPHM/Idy9W+Uv/eAqfrsJcC3atZ7erp8y+pw4EPlspXAw8xOR/TzPGpb+yLityLil8v7KyPi5ohYExF/VGd/f1T+vLoy/2OLqL2HRMSVEfHhiPhYRBwzxb6qbXx/k2078HinXh/OUNsf9xFxfURcPEV9rWxrszrtsfOhevur7Dtm1sQZaetjJCL+OSJeV96/JCJ2TlFnw35qoN3tvLGyXdS7XzPvY803raGm+m5lC39hpzuZIqQccCdwcUT8FnAacEZm/jgivgzck5mPR0Q38MvAVRFxdWb+KXAKMJSZowvdgBZ4jCL4V53M5BihR4GzKfrkwxRnOqfbloj4BYpwuDEiPgqsYW5haF7qbHL7p2VmMx8uqN3nDuA3gKOAzwBncvDYq5MoH4NT9VlE/CzwSGbuK9c7aJriwHR7Zj5V8/vvbqLOejqin+dRO9t3F8W7TP8d+MphsK8AAAVuSURBVD1gO3AEcGxEbAceoTjDcjewMiLWAi+NiPdSDNEZAB6LiFXA+4BDKd7V+v0ObW8vsDcz3w8QESdFRH9m/nFE/CnwbiArbXwnxVj8g9oI/AHFyYekGNf/7XK7dwN/WfbX7wPfyMwvRcQ15b7eswj6ifJF9k0UxwciYmWlvdW2rqV4Qb0a+L8U71J9AvgcxbC2Vnyot5MeO2cz2Qcf5eC2fwT4NEXbozxR9XlgFLiwPFnRCm3rjzJg/h1wUkTcBbwI+Ic6z5PPUfZT+Ty7FHgu8HBmXrpI2vm9iPg5Jv+m10bEDeX96yLi92ra++7yd1LbVuD7HHws+APg4tq+iIhjqXnOAH9BTd9N10HL5kwyRUCZgKdfoZxHMUb0JODvyoD8JuAXga9HxIsz87HM/CTFAW1NuZ8HgV+KiGeV+zppnl/RttIO4M0RcSQUHzCLiPMp+uDAA/oDwDvKEHYSk2F3qm0BLgNem5kbMnMDxQuJuZxJnq86m9l+pmr3+b8oxhAfOEB8DzixrOOFwNsoxiDD1H12ArCnZv/V6W4OfvV+4Kz1bCymfp6NdrbvGxQH4ecCv5iZnwFOpfiH/hPgTzLzC+W83eXPv87M91IE6FMp/plcQPGP6GHg2R3c3r8HVkfEJyLiN8v67yyXHZqZj9dp45kUn/motvH3gM9m5sWZ+ScUw5n+svxndqC/vg68LCJeRfG5kN9aDP0UEWuAN2fmJyjeEaK2vRThqNrWQeCvMvNCihcXpwCfycwPAfsbtLVZnfTYqf17V9t+KvDJzLwc+BRFwB4Arm9hQG53f/xrihfXR1IEwy9R/3nydD+VZ99XlctesYjaeTsH/0031tx/XaW9LwfuqNPW6rHg8Dp9UX3O1D7GprdQ4z/afQOuAx6gGBdzB/CnFK9Quik+FPW3FGcivl2u/zGKt7jvoHgFsqacvxb463LZ7rKj296+GfTD25j8JzMBDJV988Y66z4ErG2w7WsohhRUt30EeG6n1FnzGGi4/SweV2+smf4s8M81j5XPlHV8Ffg35fzp+uz4ct27KV6wPbsyfQXwlppt7ptt7Yupnxdj+4DrgQ8CLy2n30MxdOalFGc0jqmZ9y6Kd7OgON78B4oXQFcDqxdDe8t5z6IIPR+g+Cf/M5Rj5uu08b0ULwIPaiPFWZ6VNdO/C7yy0odHUJxl/AuKd0QXRT9RDPH7DMXZ4O9QHCOebu8Ubb2W4kzqIRQvsi+peUy17P9PBz12avug2vb3AF012+wAPrKUjh3AH1EcI26geGfywHT1eVLbT9dQfI7qhcA1i6idp9T+TSv3q+09cEw8qK0881jwjL6g8pyp7btGtyg3kiS1UER8HPheZr6nnB4F/g/FBzXXUBz0R4DzgT+n+GDxEcAngd8E3k4xZOPXge8C/zMzP7/AzWhKOaRiP8WLuhspxiG+tJz3cGZuK4eZ1LbxdzLzvCjGbj/dRopQ9DqKvvovFGcL3wRcTnFm7fzM/GlE3E0xpv/G6j46sZ8i4njg0swcKKcvpXhn4eeYbO/XKS75+HRbgXOAX6O4tNafAP8R+PcUbydvzcx3L2xLWqvOYyeY/Hu/kDptz8yfltt+rpx+qA2lz4uIuB7oB1Zk5r6a6XM5+HlyCJP9tIniBenPAt/KzCvbUftMRMSnKf62V1H+TcvhEgfuV48Lv0pxTHwXlbZWjgUX11l+DTXPGYorb7wJuDwzJ6at05AsSVpIEXEhxTtNC/XBNC0hEXE4xVjlnZl5Q7vr0dJlSJYkSZIqltMH9yRJkqSmGJIlSZKkCkOyJEmSVGFIliRJkioMyZIkSVKFIVmSJEmqMCRLkiRJFYZkSZIkqeL/A7lULGQqVdMcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for config in configs:\n",
    "    df[config.name] = config.medRs\n",
    "    print(f'\\n {config.name}')\n",
    "    print(config.medRs.mean(), config.medRs.std())\n",
    "    for r,v in config.recalls.items():\n",
    "        print(r,v)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "df.plot.box(subplots=False, figsize=(12,6))\n",
    "ticks = range(1, len(configs)+1)\n",
    "labels = [r'${}$'.format(x.name) for x in configs]\n",
    "plt.xticks(ticks, labels)\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = \"10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
